---
tags  : [Phonology, Ling 581]
category: [Phonology, Ling 581]
---

## Readings

## A Guide to Analysis in MaxEnt Optimality Theory Chapters 1 and 2

## My thoughts 
- For some data that is quantitative (“定量的”“数量化的”。也就是说，这些语言学数据是以数量、频率、百分比、统计指标等形式呈现或需要用数值方法来分析的), which means there might not be a rigorous analysis for this type of data, but MaxEnt grammar is suitable for this purpose

## My Questions
- Why are probabilities suitable for gradient phenomena?
- MaxEnt 为什么可以 Provide concrete interpretations for linguistic experiments？
- MaxEnt offers both quantitatively precise analysis as well as statistical testing, which permits each constraint in the analysis to be checked for whether it is making a meaningful contribution
  - 突然感觉！MaxEnt 是不是可以用来test syntactic binding constraint for reflexive？如果我已经有英语和中文的proportion data的话 （chapter 8 有syntax example）
- The lenition process is confined to function words. When a /p/ appears in a content word, it never alternates. Why does lenition only appear in a function word, but not in a content word?
  - 语法地位与韵律结构：功能词通常在韵律上处于弱势位置（非重读、黏附在相邻词上、处在所谓的“clitic group”里），弱势位置更容易发生lenition。
  -  信息结构与突出度：内容词承载语义信息，往往带重读，处在“强”位置，音段更稳定；功能词信息量低、常非重读，容易被语流中的同化、弱化影响。
- How do people set the original constraint weights in the classical harmonic grammar?


  
## Chapter 1: Purpose and orientation
- MaxEnt Grammars are a formal apparatus for constraint-based linguistics, and an outgrowth of OT, which works on probabilities (suitable for gradient phenomena)
  - free variation = multiple outputs from a single input
  - gradient well-formedness (syntactic judgments or phonotactics
  - Matchup of native speaker judgments
    - to statistical patterns in their languages
  - Provide concrete interpretations for linguistic experiments

## 1.2 MaxEnt as tool vs. theory
2 prespectives
  1. practical = need an analytical tool to help scholars to deal with quantitative linguistic data
  2. theory = MaxEnt can extract from the theory's basic principles through reasoning and testing against language-particular phenomena and typology

## 1.3 Why is it called “MaxEnt”?
- Maximum Entropy “最大熵”在语言学中指的是 Goldwater 和 Johnson 在 2003  提出的OT中附加的数学工具，旨在使其具有概率性
- The OT architecture coupled with the MaxEnt math

## Chapter 2: The analysis of variation in outputs
2.1 Analyzing the K1 language in Classical OT

- K1 lenition (stops and other obstruent sounds take on weaker degrees of closure)
  - “Lenition” 是音变学里的一个术语，中文常译作“弱化”“软化”或“音位弱化”。它指的是辅音在特定语音环境或语法环境中由“强”向“弱”方向变化的过程。
  - 常见的弱化路径（强 → 弱，大致方向）：塞音 p t k b d g → 擦音 f θ s v ð z（去完全闭塞，变成摩擦）
  - example:
    - 威尔士语：词首弱化（mutation），如 “cath” → “gath”（根据语法环境）
    - Spanish /bdg/ to [βðɣ]
    - English /td/ to the tap [ɾ]
  - common process across languages
- The underlying phoneme /p/ in the non-phrase-initial position is normally lenited to the homorganic glide [w]
  <img width="508" height="183" alt="Screenshot 2025-08-25 at 19 03 15" src="https://github.com/user-attachments/assets/3663e00c-13f6-4590-8f4d-972fd608b72d" />

  - 2 complications
    1. lenition is blocked when /p/ is preceded by a nasal 
       - common for nasals (also stops) to disfavor nonstop articulations immediately after them
       - example: Spanish, with lenition of /b/ to [β] in many contexts, does not lenite /b/ after [m], [bomba] ‘bomb or pump’
       - in K1: /ʃit-͡tʃʰəx-sən pai-sən/ → [ʃit͡tʃʰəxsən paisən]
    2. lenition process is confined to function words, when a /p/ appears in a content word, it never alternates，弱化只发生在虚词上（功能词，如冠词、介词、代词、助词等），当音位 /p/ 出现在实词（内容词，如名词、动词、形容词等）里时，它不会发生交替（不弱化、不变成别的音）
       - in K1: /ɔrʊɮ-əx pɔɮəm-͡tʃtʰɔi / → [ɔrʊɮəx pɔɮəm͡tʃtʰɔi]

<img width="526" height="251" alt="Screenshot 2025-08-25 at 19 02 56" src="https://github.com/user-attachments/assets/8dd2476d-7071-41b0-9623-ad9a95fd4559" />

- Many controversial issues for MaxEnt are controversial for OT as well
  
Classic OT
- Aim: search for a particular candidate output that best satisfies a ranked hierarchy of phonological constraints
  - Optimal candidate = unique output of the grammar
  - Function GEN = provides a full set of candidates in a finite set of all possible strings
  - Function EVAL takes in a phonological input (the candidates of GEN) with the ranked constraint set, and outputs a winner
  - Candidate selection
    - The ranked constraint C treats candidates 1 and 2 differently, is violated more times by 2 than 2, then 2 cannot be the winner

 ## Example of OT in K1
 - constraints and rankings
 - IDENT(sonorant) = output correspondents of an input [alpha sonorant] segment in the content words are also [alpha sonorant]
   - positional faithfulness constraints = distinguish content from function words
   - faithfulness constraints penalize candidates that change something in the input
  - AGREE(continuant) = penalizes segment sequences that differ in a particular feature, penalize *[mw] = adjacent segments should have the same value for [continuant]
 - *p = assign a violation to every non-phrase-initial p
<img width="677" height="498" alt="Screenshot 2025-08-26 at 10 17 40" src="https://github.com/user-attachments/assets/66bcc922-e18d-4d2d-8e59-55747315bea1" />

## OT has been extremely influential in phonological theory, adopted by many who had previously made use of the rule-based framework of SPE (Chomsky and Halle 1968)
1. scientifically sensible goal, separation between Markedness and Faithfulness constraints 
   - Markedness principles that militate against particular surface forms
   - Faithfulness constraints that atomize the set of ways in which surface forms can differ from their underlying forms
   - OT 把“问题”（标记性）与“解决方式的代价”（忠实性）分开，通过约束排序统一解释：同一个标记性压力，为什么在不同语言/环境里会得到不同的表面修复，有时甚至阻止变化，有时促成变化。这种抽象与可比性是相较“把问题和解决打包在一条规则里”的规则式理论的主要优势。
2. Make language-specific phonology analysis more responsible to typology
   - when things go well with OT, we found a very detailed language-specific analysis can be reduced to a language-specifc ranking of principles with cross-linguistic support

## 2.2 K2 and free variation
- The process of /p/ lenition is optional in K2
- it's okay to have /p/ lenition and it's okay not to have /p/ lenition
  - free variation = no meaning difference arises
- OT's approach: instead of having ranked constraints in grammar, we let certain constraint pairs be ranked freely
  - dotted vertical line = separating the freely ranked constriants (IDENT(sonorant), AGREE(continuant))

<img width="696" height="315" alt="Screenshot 2025-08-26 at 11 44 16" src="https://github.com/user-attachments/assets/160b2c42-40d4-4a0c-9100-32097869d545" />

<img width="689" height="213" alt="Screenshot 2025-08-26 at 11 45 43" src="https://github.com/user-attachments/assets/3e19b460-2091-4711-a17c-0d88e01c7b1b" />

## 2.3 The interpretation of free ranking as probability with partially ranked constraints

Theory of Partially Ranked Constraints
- each group of constraints delimited by solid lines = constraint stratum
  <img width="291" height="186" alt="Screenshot 2025-08-26 at 11 51 46" src="https://github.com/user-attachments/assets/4023b733-e031-4d1e-a294-c715f3a8e419" />
<img width="449" height="132" alt="Screenshot 2025-08-26 at 11 52 28" src="https://github.com/user-attachments/assets/932e3873-5890-43dc-80c3-315d857d413e" />
- probability assigned to any candidate under the theory of partially ranked constraints = number of rankings for which that candidate is the winner, divided by totoal number of rankings

 Probabilitistic grammar
 - unlike OT
 - grammar with partially ranked constraints
 - assigns probability values (often 0) to the members of GEN
 - adopted fro the analysis of a variety of systems with free variation


## 2.4 K3: modeling arbitrary probabilities
- Variants are not equally likely
- percentages in speech and text cover a wide range, and in these cases to be shown as stable and replicable in both corpora and in speakers' behavior in experiments
- necessary to make use of a model of phonological grammar which can represent these patterns effectively

**In K3**
- when a function word is preceded by a non-nasal sound, which includes 300 instances of [p] and 110 instances of [w]
- surface [p] and surface [w] are not equally likely
- In order to model the data of K3, we need a probabilistic grammar that can predict the exact lenition rates
<img width="567" height="232" alt="Screenshot 2025-08-26 at 12 00 59" src="https://github.com/user-attachments/assets/87b88036-1502-441e-9331-df353492fba0" />

## 2.5 MaxEnt: an introduction
Key difference between MaxEnt and OT
- Instead of using constraint ranking, MaxEnt assigns a numerical value to each constraint (weight) = strength
- solving the conspiracy problem and linking language-specific analyses to phonological typology

## 2.5.1 Preview: Classical Harmonic Grammar
- Classical Harmonic Grammar
  - constraint weights and violations are used to calculate a Harmony score for each candidate = which acts as a kind of penalty
  - outcome = least-penalized member of GEN for a given input
    - only produces one single winner, just like OT, it is not a probabilistic theory
    - the winning candidate = lowest harmoney penalty 

  <img width="640" height="324" alt="Screenshot 2025-08-26 at 12 07 59" src="https://github.com/user-attachments/assets/dd1e9848-ce71-4a83-af5a-077c2fe17435" />

<img width="703" height="482" alt="Screenshot 2025-08-26 at 12 10 57" src="https://github.com/user-attachments/assets/4962e455-a254-4f1d-93e0-9c82f896f3e2" />

<img width="681" height="290" alt="Screenshot 2025-08-26 at 12 12 33" src="https://github.com/user-attachments/assets/979eb703-a626-4dec-9115-83a465c3e1c7" />

## 2.5.2 Shifting to MaxEnt
- MaxEnt needs one more step in addition to harmonic grammar = convert the Harmony scores to probabilities
- MaxEnt is a probabilistic version of Classical Harmonic Grammar, just as the theory of partially ranked constraints is a probabilistic version of OT.

## 2 essential properties 
1. Assign a lower probability to candidates with higher Harmony penalties
2. The probability assigned to the full set of candidates for a given input will sum to one = probability distribution

## MaxEnt transformation from Harmonies to probability
- Calculating probability from harmony values in MaxEnt
  1. caulcate the Harmony of each candidate
  2. for each harmony vlaue, make it negative then exponentiate it using Euler's number e = eH (e = 2.718, MaxEnt also works well with 10)
  3. For each input, sum up the eH over all the candidates for that input = Z
  4. for each candidate, divide its eH by Z, this is called its predicted probability
  <img width="655" height="181" alt="Screenshot 2025-08-26 at 12 20 27" src="https://github.com/user-attachments/assets/5b19e0a3-2caf-46df-92ff-13bce3a72452" />
  <img width="596" height="151" alt="Screenshot 2025-08-26 at 12 20 36" src="https://github.com/user-attachments/assets/17337384-799a-4b13-a18d-f263ac3f60bc" />

The weights in (17) were chosen by hand. While it is possible to find a good fit to the data by choosing weights by hand for a very small dataset like this one, realistic analyses generally, require the weights to be fit by machine.

<img width="668" height="207" alt="Screenshot 2025-08-26 at 12 22 25" src="https://github.com/user-attachments/assets/e170e521-c34d-4caf-b27c-fcca72d0ccf3" />
