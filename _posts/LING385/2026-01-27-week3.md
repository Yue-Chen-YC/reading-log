---
tags  : [TA, Ling 385]
category: [TA, Ling 385]
---
## Week 3

summary of 3 of the 6 deepest combinationrial ideas in Ai 
- monte carlo search tree
  - language production is a sequence of decisions, putting an utterance together is a selection from many many paths
  - SGD = how to find a good estimate of something (e.g., w), pick a random estimate of w, randomly pick data to learn from a better estimate is from the data, improve estimate by reducing the discrepancy
  - dynamic programming = steps of synmaic program, make selection of a state at each stage, and found greedy algrorithm is not very good at this, we start at the end, and we are not doing greedy backwards, solve the bigger problem by breaking them to smaller problems, memorization, we are taking a memo of every path
  - how do we combine these to make ai ideas
    - reinforcement learning: MCST + DP + SGD
    - stages = word number, states = words
    - in shortest path, an agent tries to select a good policy
    - the costs are like grammatical knowledge = why is the cost grammaritical knowledge
    - D --> H, the --> dog = 7
    - D --> I, the --> eat = 10
    - if we know all the costs in the shortest path language we can use DP to speak
      - reinformcement learning is a leanring becuase we do not know the costs, but ai first need to learn the costs and how --> SGD = finding the average = much more general idea, if you try to estimate something, instead of the average of some numbers
      - SGD for compeltion og text = cloze test
      - the psychological idea that people depend on = a lot of gaps you can fill it, how to fill in and how to close, the closure principle, humans are able to compelte it, there are lot of missing gaps = idea used in psyhcology and jornalism to break people's ability to read, way to exhibiting your knowledge of English
      - SGD of language knwoedge = we want to learn costs, learning language here, we need to exhit the simialrity of dynamic programming, the knwoedlge of language is emebeed in the cost
      - we start with random cost = we are at the stage of learning at early stage
      - starting with randomness, pick a phrase from data: eat pizza, and mask the second word "pizza" they will predict the next word such as from the noun classes, but not eat eat = masking game, we are getting the estimate of the costs from the data we are learning from
      - idea of speaking, idea of dynamic programming, and idea of how to learn something if u don't know it
      - the problem is that if you are trying to have a conversation, they are grammatical, becuase they learned to go to the good path, but they need to learn more
     
  neural networks
  - what is a word to AI?
    - what is a word in our brain?  = neurons = our brains are full of neurons, which have elective activity, words are a pattern of activities of a bunch of neurons
    - in ai we will represent word with a lot of numbers representing in arithmetic the biology of word cofnition
    - dog = maybe [4,1 ...] = neron electronical activity
      - we call this a vector, and # of elements: Dimesnion
      - SGD is about numbers, now words are numbers
      - usually random for each word
      - depending on the context, they develop different representations for different things, they will do it mulitple time for the same word
      - start with random numbers and learn about the new representations

Let's add NN to get: MCTS + SGD + DP + NN
- from a word,, we predict another word
- from eat, we tried to predict another word pizza
- words are vectors, we need to convert one vector of numbers and conterting it to a different numerical vector
- take the simplest problem and solve the simple problem and apply what we learned to solve the big problem
- imagine that a word has just 2 numbers, so we can think of a word as a vector in th eplan = [1.2]
