---
tags  : [TA, Ling 385]
category: [TA, Ling 385]
---
## Week 3

summary of 3 of the 6 deepest combinationrial ideas in Ai 
- monte carlo search tree
  - language production is a sequence of decisions, putting an utterance together is a selection from many many paths
  - SGD = how to find a good estimate of something (e.g., w), pick a random estimate of w, randomly pick data to learn from a better estimate is from the data, improve estimate by reducing the discrepancy
  - dynamic programming = steps of synmaic program, make selection of a state at each stage, and found greedy algrorithm is not very good at this, we start at the end, and we are not doing greedy backwards, solve the bigger problem by breaking them to smaller problems, memorization, we are taking a memo of every path
  - how do we combine these to make ai ideas
    - reinforcement learning: MCST + DP + SGD
    - stages = word number, states = words
    - in shortest path, an agent tries to select a good policy
    - the costs are like grammatical knowledge = why is the cost grammaritical knowledge
    - D --> H, the --> dog = 7
    - D --> I, the --> eat = 10
    - if we know all the costs in the shortest path language we can use DP to speak
      - reinformcement learning is a leanring becuase we do not know the costs, but ai first need to learn the costs and how --> SGD = finding the average = much more general idea, if you try to estimate something, instead of the average of some numbers
      - SGD for compeltion og text = cloze test
      - the psychological idea that people depend on = a lot of gaps you can fill it, how to fill in and how to close, the closure principle, humans are able to compelte it, there are lot of missing gaps = idea used in psyhcology and jornalism to break people's ability to read, way to exhibiting your knowledge of English
      - SGD of language knwoedge = we want to learn costs, learning language here, we need to exhit the simialrity of dynamic programming, the knwoedlge of language is emebeed in the cost
      - we start with random cost = we are at the stage of learning at early stage
      - starting with randomness, pick a phrase from data: eat pizza, and mask the second word "pizza" they will predict the next word such as from the noun classes, but not eat eat = masking game, we are getting the estimate of the costs from the data we are learning from
      - idea of speaking, idea of dynamic programming, and idea of how to learn something if u don't know it
      - the problem is that if you are trying to have a conversation, they are grammatical, becuase they learned to go to the good path, but they need to learn more
     
  neural networks
  - what is a word to AI?
    - what is a word in our brain?  = neurons = our brains are full of neurons, which have elective activity, words are a pattern of activities of a bunch of neurons
    - in ai we will represent word with a lot of numbers representing in arithmetic the biology of word cofnition
    - dog = maybe [4,1 ...] = neron electronical activity
      - we call this a vector, and # of elements: Dimesnion
      - SGD is about numbers, now words are numbers
      - usually random for each word
      - depending on the context, they develop different representations for different things, they will do it mulitple time for the same word
      - start with random numbers and learn about the new representations

Let's add NN to get: MCTS + SGD + DP + NN
- from a word,, we predict another word
- from eat, we tried to predict another word pizza
- words are vectors, we need to convert one vector of numbers and conterting it to a different numerical vector
- take the simplest problem and solve the simple problem and apply what we learned to solve the big problem
- imagine that a word has just 2 numbers, so we can think of a word as a vector in th eplan = [1.2]

inner production
- which 2 patterns are more similar, the inner product for discovering similarity of 2 patterns: multoply the two patterns component-wise, and all the products
- <a,b>: (2 * 2) + (-1 * 1) + (2 * -1) = 1
- <a,c>: 2 * 1 - 1* 0 + 2 * -1 = 0
- <b,c>: 2 * 1 + 1*0 + -1*1 = 3
  - a and b and a and c are different to each other and b and c are similar to each other
  - 0 or near 0, if the inner product is a positive number = orthogarinity
  - the closer the inner prduct is to 0 = the similar these two products are
  - the higher in the magnitaude th einner producction is, the more similar 2 patterns are
    - how we are going to infer from eat to pizza

  Why does inner product capture similarity?
  - when two patterns are similar, having the same shape, they tend to have postivies corresponding to positives, and negatives to nagatives
  - when two patterns are similar, or have the same shape, if we do the productions, we got a lot of positives
  - capture the similarity despate the differences
  - the inner production multiplicaiton when patterns as simila to be added then lead to a big number and that's similiarity
  - for dissmilar patterns, some products will be plus and othe negative, so when we add them all up, we get a number near 0
 
  why we need to calcuate the inner production?
  - inference, we want a mathmatical model of inference of 1,2 and mins 1,3
  - if we can find such mathm, it's the math of having knowledge, thinking and inferring
  - for a system to know something is to have some numbers, the system's knwoedlge will be in numbers, in human brains' knwoedlge is a bunch of neurons
  - words are numbers, also knwoedlge is numbers, in our brains, the words are in activation of nuerons
  - long term poteniation = how brains learn these numbers
  - the system must have knwoedlge, then the knwoedlge must have numbers = in the vectors [1, -1] and [1,]
  - weights of a neural network, they constitute knowedlge = parameters are also weights (biases)
  - how can we use these knowledge to infer 1, -3 to 1,2
  - now apply the second knwoedlge vector of weights [1,1] to [1,2]
  - to turn 2 numbers to 2 numbers we need 4 four numbers = 122888 numbers to represent word, how many weights does it need? 122888 square of weights
  - how do you think these systems would learn these numbers if I need to give the system> (SGD or DP)
  - take the and change the weights a little bit and i can predict cat from the
    - contextual vectors = attention is all you need
    - weight matrix = one vector of weights and another vector of weights 
