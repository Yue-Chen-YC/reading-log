---
tags  : [TA, Ling 385]
category: [TA, Ling 385]
---
## Week 1
AI… where we are, and where we were …
What makes language complex?
Most General Solutions:
- Averaging via Stochastic Gradient Descent
    - scaled by words, averaging via words
- Dynamic Programming gives memory and forseight
    - gives ai system memory and future
 
why is ai interesting to learn about?
- will ai ruin the plant or save the planet?
- extremes
- economic system stability = news about ai chaging the whole field of market goes up and down
  -  affecting so many things
    - technologies, government, entertainment, education, science
    - Tech: Self-driving cars are all over LA. Science: 2 Nobels in 24!
      - Positive: Novel immunotherapies, educational potential …
      - Negatve: Massive job loss, end of creativity, Environmental costs, Real/Unreal…
        - you learn how to think creativity by reading more books and papers
        - are you a creative thinker
        - what is real and what is unreal? ai generated youtube videos
- educational potential
  - ai can help students into education, if u use the potential of such systems, you can learn more things
  - textbook and newspapers can contain worng information

 gerative ai, llms and all other ais are all the same.

 ai started wth a language translation system in 2017


 importance of how language works and difficulties of computing languages, we can understand how they work 
 - maybe drive better

Scientific History of AI: ideas of historical history of AI
- Leibniz (1666): “Everything done by our mind is a computation”
  - thinking machines, how can u construct a machine that thinks = computation
- Lovelace (1842): “the machine must therefore have the additional requisite of executing by itself all the successive operations required for the solution of a problem proposed to it, when once the primitive numerical data for this same problem have been introduced. Therefore, since from the moment that the nature of the calculation to be executed or of the problem to be resolved have been indicated to it, the machine is, by its own intrinsic power, of itself to go through all the intermediate operations which lead to the proposed result”
  - actually algorithem of Stochastic Gradient Descent
- Freud (1895): Project for a Scientific Psychology
  - furnish with a psychology which is a natural science as chemistry and phsyics
    - quantity = activation of a neuron
    - assumed that the material particles in question are the neurones
   
McCulloch and Pitts (1943)
- first complete working ideas of neuron-network
- idea of aritificial neuron network in human brains
- Von Neumann (1945) --> first mauel of develop a computer
- 2 approaches of cognition
  - connectionist
  - artifical network

Margret Hamilton (1968): Software
- notion of software
- until 1968, worked on mission to the moon
- write a program for a system that to work required a lot of duplication of codes
- if u understand the idea of software, u only needs some of the codes

Rumelhart, McClelland, Hinton, etc. (1986) in CA
- parallel distributed processing
  - name then for neural networks
  - microstrucutre of cognition, you can program in your computer as a software

2024: Nobel prizes in Physics and Chemistry for AI
- science is about ideas benefiting each other, work done in physics help in aritifical nureon network to think

Technological History of AI
- 2017: Transformers for English --> German/French, Words = Numbers
  - attention is all you need
  - Transformers = how to translate english sentences to german and french sentence
  - translation by machine
  - how to make words, they have meanings, they had to have a specific way of a bruach of numbers = a vector
  - how to convert the discourse in one language to another language
  - in order to do that, words need to become numbers
  - if each word is a vector of number
- 2020: Patches of images -->  Words, Vision transformers
  - what if about images = picsles
  - we can take different patches, and your brain are seeing these patches as meaning
  - what is we take image and treat each patch as a word
  - translate image to something like this is a dog, this is a cat
  - image recognition system
  - vision transformers
- 2020: Speech pressures --> Words, Voice Synthesis and Recognition
  - speech are chages of pressure in the mouth, we can take small amount of pressure change
  - we can treat them as problems of translating speech pressures in the text
  - siri, alexas 
- 2020: Amino Acids --> Words, Protein Language Models (2024 Nobel)
  - a setencnes is one word after another
  - there are 20 Amino Acids, they make tissues
  - a protein is a important aspect of living things, the sequence can be a lot
  - what's the shape of amino acids, nail, hair, tissues
- 2022: Artificial Reasoning Systems
  - reasoning = search enginein will not do reasoning 
- 2026: We still don’t understand the systems of 2017

Goals of Ling 385
- Deep computational intuition for how AI systems, what we know. Appreciate what we don’t understand.
- An appreciation for language structure, and how it gives us hints for what AI systems are trying to do.
- I will be teaching a little programming, just so you know what programming is like. The skill you now need is not programming from scratch, but how to interact with AI to generate programs, and we will do lots of that.
  - how do you talk to an ai system for it to produce the information you wanted 
- To obtain an appreciation for how deeply connected scientific disciplines are, and how deeply connected technology is to science

Why is language complicated? Example: Producing a Sentence
- a child takes years to put a sentence together
- imagine we have a person, and the person is producing a sentence
  - they need to select the word forms from the entire vocabulry
  - sentences will contain many words, words after another words
  - if there are just two words in English, how many sentences I can creat?
    - 2 x 2 = 4 words
    - Just made a Choice among 100,000,000 of 2-word sequences in English
    - By the 20’th word, we would be making a choice among 1080, Sequences, around the number of atoms in the universe!
      - 10 to the 80s, atoms in the universe
      - language is hard and image is hard as well
      - what makes vision very difficult
      - talking is hard, because we need to select based on so many possibilities
     
- We don’t have to select from every possible sequence of words
- For example: “a the a the a the…” is not a possible sentence
  - not a sentence in English, we need to select based on the much smaller space or there are a lot of restractions, an ai system must have learned
  - changed from english word to turkish word
  - what makes a sequence possible and what not
- True… And many many other sequences are indeed impossible  
- E.g. “…won gave…”
- Right?
- Gotcha!
- “the guy who won gave the money…”
  - you can not say I won gave
  - more and more patterns, you have to know
  - allow the to follow a or allow a to follow the
- What’s a possible sequence of words is actually a very complicated aspect of English 
- An AI system needs to learn these complexities within sentences and across sentences for it to produce correct and creative responses with 100’ of words/sentences!
  - learn these complexities within sentences and across sentences in just memerizing these
  - if u say the, what's the most common word after the 

How do AI systems learn to be correct/creativein reasoning though pages of text?
- They are exposed to trillions of words of language that have implicit (and sometimes explicit) indications of how humans reason.
- implicit: a system will take wikipedia, when someone is writing a sentence, you construct high depdent strucutres
- explicit
- chain of thoughts 
- Each sentence/discourse is different, but many patterns of how people speak and reason are present, regardless of whether the topic is politics, sports, or plays!
- AI systems, kind of, average the knowledge from amy sentences and discourses, so that they can talk about (just about) anything, just like humans.
- So we will begin our journey at a very humble place: averaging

## Week 1 lecture 2
why is language complicated?
- producing a sentence
- select one out of 100000 words in english, by the time you get to the second word, it's already 10000000 words, we will multiply by 10 thousand, by 20 words, we are already selecting ten
- atoms in the entire universe
    -  linguistics are pround at speaker will take a path of the network, that no one ever take it before
    -  most of the sentences are part of conversations
    -  for most of the things in daily bases there are pathes
    -  own use of ai, see producing pathes that no one ever taken, creativeity
    -  many people believe, ai is not interesting, the singificant thing is given of conversations you have, and believe that pathes are taken
    -  can u use latex to write a report
    -  able to join ideas across domians of leanring, and create high sctrecutured materials
 
agentic multi-model 
- they will create the plan for you

large behavior models 
- they train them on behaivors
- robot = that never been said before

so many pathes that are complitly impossible 
- there are very complicated rules, how you are retrive it

How do Ai systems learn be to correct/creative in reasoning though pages of text?
- they are exposed to trillions of words of language that have implicit and sometimes explicit indications of how human reason
- able to learn something about mathmatics from data
    - you can give a bunch of data and figure out how to solve mathamtic problems
    - these systems will be told to do mathmatic reasoning = someitmes people write down these reasoning as well
    - explicit = what if we ask someone about how you solve this mathimatic problem = raw text and language
    - what if we take this answer and add a special token to the task = questions? the system is asked of a question
    - after question mark we can say end question
    - answers are specital token
    - step one is a special token
    - get the sysmtem get a learn = they end up learning the reasoning people sue
    - systems can learn with these argumentive text, they are given a lot of raw text and argumative text
    - implicitc answers = the ai system need to average one person's solution to a question, a second person's soculution to a second question
 
averaging with language?
- we can think about a group of numbers as inconsistent, each telling us something different
- image we know the sentnces
    - shri wanted to play the piano
    - pri tried to climb the new gymn wall
    - this player managed to score well in poker
        - the construction in english is control = what verbs work like this and how to use them
        - ai should also have this knowedlge as you have, all verbs are different but there are something common between them
        - being able to given examples but also extract their essenses
     
Stochastic gradient descent (SGD)
- What's the average = essence of a string of numbers
- pick any random number x as the estimate of average and we will imporve it by observation
- x[0] = 130
- algoprithm = the intial data is 130
- at the beigning of the algoprithem is time 0
- estimate is 130
- randomly pick a number from the list, we will call it d = data
- d[0] = 98, the number we learn from = how far the data is from the estmate
    - if we are far from the data, we should get closer to the data
    - we need to figure out the mistake = loss = 130 - 98 = 32
    - learning is always trying to minize the loss, we are trying to get closer from the data we are seeking
    - improve from the estmate, take a little bit of the loss, take 10% of 32
    - x[1] = x[0] - 10% of 32
    - take it away from the current estimate and subtract it from current estimate
    - x[1] = 130 - 32 x 0.1 = 126.8
    - repeat: pick another datapoint and imfprove x[1] --> x[2], relearn from the same data over and over again
        - when we are done, we make an intial estimate/guess and used data to improve the estimate
        - averaging is more about intelligence
        - use averaging using SGD, the average faces of a demonsion
        - stochastic (random) gradient (loss-related) descent (reduce)
        - the estimate of time 1 = the estimate of previous time - learning rate times the loss (current estimate what the data exmained)
            - current estimate is based on previous estimate


learning rate 
- inisital conditions and do a produce over and over agian
- the very frist number here at the time 0 = epchoes of learning
- adaptive learning = now we assume to keep it fixed
    - 1% of how smooth it is the learning is
    - oplization thoery sachedio that is fixed
    - leearning rate goes up, we learn faster but less accuratly
    - if learning rate goes down, we learn slower but more accurate
        - takes a lot of energy and time to be adaptive
        - fundamental tradoff of learning and energineering
     
there are two fundamental algorithms that come togher in modern ai
- SGD is one of them
- dynmic programing
- why do we need this alogrithm
    - how does ai traverse humanougous path spaces = spaces have many posisble path
    - how do u go through path = shortest path problem = language production
    - to minizie the overall cost of the path
    - every stage have several stage = greedy algorithm = gives u a solution that is not the optlimzation sloution
        - exhastive algorithm 


